# -*- coding: utf-8 -*-
"""Ensemble_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qNgWBrSyg8vTQ0RW0LQMeKSKOTAdHz-d
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import SelectKBest, f_classif

from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import mutual_info_classif
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier

data = pd.read_csv('/content/14channels_Combined.csv')

print(data.head(20))

# Assuming 'data' is your DataFrame containing the dataset
non_numeric_columns = data.select_dtypes(exclude=['number']).columns.tolist()
print("Non-numeric columns:", non_numeric_columns)

X=data.drop(['Label'],axis=1)
y=data['Label']

print(X)

k = 10  # Top 10 features to select (adjust based on your dataset size)

# Select the top k features using ANOVA F-value
selector = SelectKBest(score_func=f_classif, k=k)
X_selected = selector.fit_transform(X, y)  # Select the top k features

# Get the scores for each feature and their corresponding p-values
feature_scores = selector.scores_  # ANOVA F-values for each feature
feature_p_values = selector.pvalues_  # P-values for each feature

# Get the indices of the selected features
selected_feature_indices = selector.get_support(indices=True)

# Get the names of the selected features
selected_feature_names = X.columns[selected_feature_indices]

print("Selected features:")
print(selected_feature_names)

print("Feature scores:")
print(feature_scores)

print("Feature p-values:")
print(feature_p_values)

X

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)

number_of_records = X_train.shape[0]

print("Number of records in X_train:", number_of_records)

smote = SMOTE(random_state=0)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
print("Class distribution before SMOTE:")
print(y_train.value_counts())
print("Class distribution after SMOTE:")
print(pd.Series(y_train_resampled).value_counts())

# Define multiple base classifiers for the stacking ensemble
from sklearn.tree import DecisionTreeClassifier  # import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
base_classifiers = [
    ('decision_tree', DecisionTreeClassifier(random_state=42)),
    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('xgboost', XGBClassifier(random_state=42)),
    ('gradient_boosting', GradientBoostingClassifier(random_state=42))
]

# Define the stacking ensemble with Logistic Regression as the final estimator
stacked_classifier = StackingClassifier(
    estimators=base_classifiers,
    final_estimator=LogisticRegression()
)

from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from scipy.stats import randint, uniform
import numpy as np

# Base classifier
rf = RandomForestClassifier(random_state=0)

# Random search parameter distributions
param_dist = {
    'n_estimators': randint(50, 150),
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': randint(10, 30),
    'min_samples_split': randint(2, 8),
    'min_samples_leaf': randint(1, 4),
    'bootstrap': [True, False]
}

# Randomized search for initial hyperparameter tuning
random_search = RandomizedSearchCV(
    rf,
    param_distributions=param_dist,
    n_iter=100,  # Number of random search iterations
    cv=5,
    random_state=0,
    n_jobs=-1,
    verbose=1
)

random_search.fit(X_train, y_train)

# Extract the best parameters from the random search
best_params = random_search.best_params_
print("Best parameters from RandomizedSearchCV:", best_params)

# Best parameters obtained from RandomizedSearchCV
best_params = {'bootstrap': True, 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 6, 'n_estimators':109}

# Create a grid of hyperparameters for GridSearchCV based on the best parameters
grid_params = {
    'n_estimators': [best_params['n_estimators'] - 10, best_params['n_estimators'], best_params['n_estimators'] + 10],  # Around 125
    'max_features': [best_params['max_features']],  # 'log2'
    'max_depth': [best_params['max_depth'] - 2, best_params['max_depth'], best_params['max_depth'] + 2],  # Around 10
    'min_samples_split': [best_params['min_samples_split']],  # 5
    'min_samples_leaf': [best_params['min_samples_leaf']],  # 1
    'bootstrap': [best_params['bootstrap']]  # False
}

# Grid search for fine-tuning
grid_search = GridSearchCV(
    rf,
    param_grid=grid_params,
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

# Build the final model with the best parameters from the grid search
best_rf = RandomForestClassifier(**grid_search.best_params_, random_state=0)

# Fit the final model
best_rf.fit(X_train, y_train)

# Get training and testing accuracy
from sklearn.metrics import accuracy_score

y_train_pred = best_rf.predict(X_train)
y_test_pred = best_rf.predict(X_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

print("Training accuracy:", train_accuracy*100)
print("Testing accuracy:", test_accuracy*100)

from sklearn.datasets import make_classification
from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay
from sklearn.metrics import confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
# Generate the confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_rf.classes_)

# Display the confusion matrix
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Generate the ROC curve
y_prob = best_rf.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

